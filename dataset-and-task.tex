\subsection{Datasets}
Flickr8k is a widely used benchmark dataset for image captioning, containing 8,000 images and 40,000 captions \cite{hodosh2013framing}. Each image in the dataset is accompanied by five captions written by different annotators, which ensures a diversity of descriptions for each image. The dataset was initially designed to assess the effectiveness of image captioning models and has since been widely adopted in research for exploring and contrasting various image captioning techniques. This dataset is the foundation of our baseline model.

ActivityNet is a benchmark dataset for human activity understanding that contains 19,994 untrimmed videos from 203 activity classes \cite{activitynet}. It is divided into training, validation, and testing subsets and includes 849 hours of videos in total. It is the largest dataset for temporal activity detection and features a hierarchical annotation framework and multiple modalities for multi-modal learning and fusion. The main purpose of ActivityNet is to provide a standard reference for evaluating algorithms that aim to comprehend human activities and creating video captioning models.

\subsection{Task}
At a broad level, our study performs a comprehensive 5-way comparison of video description models on the ActivityNet dataset. In particular, we compare our crude baseline model for video description generation with 4 advanced transformer-based end-to-end models, namely, DenseCap \cite{johnson2016densecap}, MART \cite{liu2020mobile}, PDVC \cite{le2021pdvc}, and SwinBERT \cite{lin2022swinbert}. ChatGPT is a large language model developed by OpenAI, based on the GPT-3 architecture \cite{Brown2020LanguageMA}. It uses deep neural networks to generate human-like responses to natural language input. The output video descriptions from each model are passed through ChatGPT to improve their quality and fluency. This serves as our new benchmark for performance evaluation. The goal is to identify the most effective method for producing high-quality video summaries that are coherent and semantically meaningful. 

The subsequent sections of this report cover the methodologies, experiments conducted, and metrics used to evaluate the results.