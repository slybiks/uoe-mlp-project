Video captioning has been extensively studied in recent years, with various approaches proposed such as using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for content encoding and decoding \cite{yu2016video}. Initial methods involved encoding video frames into a fixed-length vector via CNNs and subsequently decoding it into a natural language sentence with RNNs \cite{pan2016hierarchical}. Attention mechanisms have also been introduced to improve the performance of these models by focusing on they key regions of videos \cite{zhou2018end}. Transfer learning techniques have also been applied to improve the efficiency of video captioning models by pre-training them on large-scale image or video datasets \cite{mahajan2018exploring}. More recent studies have explored the use of transformer-based architectures, such as the popular BERT model, for video captioning tasks \cite{kim2020dense}. Transformer-based models involve understanding and modeling spatial-temporal dynamics in videos and generating output sequences of words. These models learn from offline extracted video representations using feature extractors trained on image/video understanding tasks to extract appearance (2D) and motion features (3D) from densely sampled video frames \cite{Lei2021LessIM,Luo2020UniViLMAU}. However, end-to-end training with this approach is computationally intensive. In addition, some research has focused on temporal event proposal such as boundary-based methods that use salient frames to create proposals for identifying event-containing segments in untrimmed videos. However, these methods rely on binary mask vectors and scalar descriptiveness scores that are not informative enough to guide feature representation \cite{Li2018JointlyLA,Singh2022V2TVT}.