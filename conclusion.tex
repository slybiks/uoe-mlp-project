In this study, we evaluated the performance of 5 video captioning algorithms, including a naive baseline model, by using ChatGPT to assess the quality of their generated captions. The evaluation metrics included Cross-Encoders, ROUGE, METEOR, and SARI. While the summaries produced by SwinBERT were found to be the most efficient, the SARI scores revealed some unexpected results. The worst-performing model, PDVC, performed surprisingly well on this metric, while DenseCap and SwinBERT, which performed well on other metrics, scored lower on SARI. Overall, the results suggested that all of the model performances were poor and at par with each other on the ActivityNet dataset, highlighting the need for more challenging evaluations to test their capabilities. One key observation was that splitting videos had a negative impact on the performance of SwinBERT, possibly because the splits were made based on regular time intervals rather than exact event boundaries.

Some possible future directions for the experiments conducted in this project include evaluating other video captioning techniques on the ActivityNet dataset, such as action localisation models that label and localise specific actions or activities in a video at their respective start and end times. Models like BMN and BSN, available in the popular MMAction2 computer vision framework \cite{mmaction}, produce dense feature encodings of the spatio-temporal information in video frames, which can be transformed into captions and coherent video descriptions using any of the transformer-based models we explored. Further studies could also investigate the ethical implications and fairness of the generated video descriptions, which is an important practical aspect to consider in most AI-based applications today. The data used to train the models, evaluation metrics, or ChatGPT's responses may be biased towards certain types of content or languages and could result in inappropriate profiling of individuals or groups in the videos based on their ethnicity, race, religion, or social status. By incorporating such ethical and data protection considerations, the project can contribute to the development of responsible and fair video summarisation techniques that benefit everyone. We also propose to experiment with multi-modal data such as audio to see if this can improve performance. Another natural extension would be to test our results on the recently released GPT-4 model.